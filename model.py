# -*- coding: utf-8 -*-
"""手書き電卓 のコピー

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UwzuJ_ePfQZQphqp75KMwYwRE0Iu2C8a
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
!pip install tensorflow


# MNISTデータセットの読み込み
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# データの確認
print(f"Training data shape: {x_train.shape}")  # (60000, 28, 28)
print(f"Test data shape: {x_test.shape}")      # (10000, 28, 28)

# 画像の表示
plt.imshow(x_train[0], cmap='gray')
plt.title(f"Label: {y_train[0]}")
plt.show()


# 四次元の行列にする
def im2col(input_data, filter_h, filter_w, stride_h=1, stride_w=1, pad_h=0, pad_w=0):
  if stride_h == 0 or stride_w == 0:
    raise ValueError("Stride values must be greater than 0")
  N, C, H, W = input_data.shape
  out_h = (H + 2 * pad_h - filter_h) // stride_h + 1
  out_w = (W + 2 * pad_w - filter_w) // stride_w + 1

  img = np.pad(input_data, [(0, 0), (0, 0),
               (pad_h, pad_h), (pad_w, pad_w)], 'constant')
  col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))

  for y in range(filter_h):
    y_max = y + stride_h * out_h
    for x in range(filter_w):
      x_max = x + stride_w * out_w
      col[:, :, y, x, :, :] = img[:, :, y:y_max:stride_h, x:x_max:stride_w]

  col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)
  return col, out_h, out_w

# class Convolution:
#   def __init__(self,W,b,stride=1,pad=0):
#     self.W=W #横
#     self.b=b #高さ
#     self.stride=stride #ストライド
#     self.pad=pad #パディング


#   def forward(self,x):
#     FN,C,FH,FW=self.W.shape
#     N,C,H,W=x.shape
#     out_h=int(1+(H+2*self.pad-FH)/self.stride) #出力の縦のサイズを計算
#     out_w=int(1+(W+2*self.pad-FW)/self.stride) #出力の横のサイズを計算

#     col=im2col(x,FH,FW,self.stride,self.pad)
#     col_W=self.W.reshape(FN,-1).T #フィルターの展開
#     out=out.reshape(N,out_h,out_w,-1).transpose(0,3,1,2)
#     return out

class Convolution:
  def __init__(self, W, b, stride=1, pad=0):
    self.W = W  # フィルターの重み
    self.b = b  # バイアス
    self.stride = max(1, stride)  # ストライド
    self.pad = pad  # パディング

  def forward(self, x):
    FN, C, FH, FW = self.W.shape
    N, C, H, W = x.shape
    out_h = int(1 + (H + 2 * self.pad - FH) / self.stride)  # 出力の縦サイズ
    out_w = int(1 + (W + 2 * self.pad - FW) / self.stride)  # 出力の横サイズ

    col, _, _ = im2col(x, FH, FW, self.stride, self.stride, self.pad, self.pad)
    col_W = self.W.reshape(FN, -1).T  # フィルターの展開

    # 畳み込みの計算
    out = np.dot(col, col_W) + self.b  # 行列の積を計算し、バイアスを加える

    # 出力の形を変形
    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)
    return out

# class Pooling:
#   def __init__(self,pool_h,pool_w,stride=1,pad=0):
#     self.pool_h=pool_h
#     self.pool_w=pool_w
#     # self.stride=stride
#     self.stride = 1
#     self.pad=pad

  # def forward(self,x):
  #   print(f"Pooling forward called. Stride: {self.stride}")  # 追加
  #   if self.stride == 0:
  #       raise ValueError("Pooling stride should not be zero!")  # 明示的なエラー
  #   N,C,H,W=x.shape
  #   out_h=int(1+(H-2*self.pool_h)/self.stride) #出力の縦のサイズを計算
  #   out_w=int(1+(W-2*self.pool_w)/self.stride) #出力の横のサイズを計算

  #   #展開
  #   # col=im2col(x,self.pool_h,self.pool_w,self.stride,self.pad)
  #   col = im2col(x, self.pool_h, self.pool_w, stride_h=1, stride_w=1, pad_h=self.pad, pad_w=self.pad)  # ここを修正
  #   col=col.reshape(-1,self.pool_h*self.pool_w)

  #   #最大値 列向き
  #   out=np.max(col,axis=1)

  #   #整形
  #   out=out.reshape(N,out_h,out_w,C).transpose(0,3,2,1)
  #   return out

class Pooling:
  def __init__(self, pool_h, pool_w, pad=0):
    self.pool_h = pool_h
    self.pool_w = pool_w
    self.stride = 1  # ここで固定
    self.pad = pad

  def forward(self, x):  # ← ここが消えていないか確認
    N, C, H, W = x.shape

    # 修正: プーリングの出力サイズ計算
    out_h = int(1 + (H - self.pool_h) / self.stride)
    out_w = int(1 + (W - self.pool_w) / self.stride)

    print(
        f"Pooling.forward called: N={N}, C={C}, H={H}, W={W}, out_h={out_h}, out_w={out_w}")

    col, _, _ = im2col(x, self.pool_h, self.pool_w, stride_h=1,
                       stride_w=1, pad_h=self.pad, pad_w=self.pad)
    col = col.reshape(-1, self.pool_h * self.pool_w)

    out = np.max(col, axis=1)

    print(f"out.shape before reshape: {out.shape}")

    # 修正: reshapeの形が合っているか確認
    out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 2, 1)
    return out

class Affine:
  def __init__(self, W, b):
    self.W = W  # 重み
    self.b = b  # バイアス
    self.x = None  # 入力
    self.dW = None  # 重みの勾配
    self.db = None  # バイアスの勾配

  def forward(self, x):
    """順伝播"""
    self.x = x  # 入力を保存
    return np.dot(x, self.W) + self.b

  def backward(self, dout):
    """逆伝播"""
    self.dW = np.dot(self.x.T, dout)  # 重みの勾配
    self.db = np.sum(dout, axis=0)  # バイアスの勾配
    dx = np.dot(dout, self.W.T)  # 入力の勾配
    return dx

class ReLU:
  def __init__(self):
    self.mask = None  # 負の値の位置を記録する

  def forward(self, x):
    self.mask = (x <= 0)  # xが0以下の部分をTrueにする
    out = x.copy()
    out[self.mask] = 0  # 負の値を0にする
    return out

  def backward(self, dout):
    dout[self.mask] = 0  # 逆伝播では、元の値が0以下だった部分の勾配を0にする
    return dout

def softmax(x):
  x = x - np.max(x, axis=1, keepdims=True)  # オーバーフロー対策
  exp_x = np.exp(x)
  return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def cross_entropy_error(y, t):
  if y.ndim == 1:
    t = t.reshape(1, -1)
    y = y.reshape(1, -1)

  batch_size = y.shape[0]
  return -np.sum(t * np.log(y + 1e-7)) / batch_size  # log(0)対策

class SoftmaxWithLoss:
  def __init__(self):
    self.y = None  # Softmaxの出力
    self.t = None  # 正解ラベル
    self.loss = None  # 損失

  def forward(self, x, t):
    """順伝播（Softmax適用 & 損失計算）"""
    self.t = t
    self.y = softmax(x)  # Softmaxを適用
    self.loss = cross_entropy_error(self.y, self.t)  # 損失計算
    return self.loss

  def backward(self, dout=1):
    """逆伝播（勾配計算）"""
    batch_size = self.t.shape[0]
    dx = (self.y - self.t) / batch_size  # ソフトマックス層の勾配
    return dx

# データの正規化
x_train = x_train.astype(np.float32) / 255.0
x_test = x_test.astype(np.float32) / 255.0

# チャネル次元を追加 (N(多分60000?), 28, 28) → (N, 1, 28, 28)
x_train = x_train.reshape(-1, 1, 28, 28)
x_test = x_test.reshape(-1, 1, 28, 28)

# ラベルをone-hotエンコーディング
def one_hot_encoding(labels, num_classes=10):
  return np.eye(num_classes)[labels]

t_train = one_hot_encoding(y_train, 10)
t_test = one_hot_encoding(y_test, 10)

# class CNN:
#     def __init__(self):
#         # 初期化
#         self.conv = Convolution(
#             W=np.random.randn(16, 1, 3, 3) * 0.01,  # 16個のフィルタ (3x3)
#             b=np.zeros(16),
#             stride=1,
#             pad=1
#         )
#         self.pool = Pooling(pool_h=2, pool_w=2)
#         print(f"Pooling initialized with stride: {self.pool.stride}")  # 追加
#         self.fc1_W = np.random.randn(16 * 27 * 27, 100) * 0.01  # 全結合 (14*14*16 → 100)
#         self.fc1_b = np.zeros(100)
#         self.fc2_W = np.random.randn(100, 10) * 0.01  # 出力層 (100 → 10)
#         self.fc2_b = np.zeros(10)
#         self.loss_layer = SoftmaxWithLoss()

#     def forward(self, x, t):
#         self.x = x
#         self.t = t
#         self.conv_out = self.conv.forward(x)  # 畳み込み
#         self.pool_out = self.pool.forward(self.conv_out)  # プーリング
#         self.fc_input = self.pool_out.reshape(x.shape[0], -1)  # Flatten
#         self.fc1_out = np.dot(self.fc_input, self.fc1_W) + self.fc1_b  # 全結合1
#         self.fc2_out = np.dot(self.fc1_out, self.fc2_W) + self.fc2_b  # 全結合2
#         self.loss = self.loss_layer.forward(self.fc2_out, t)  # Softmax & Loss
#         return self.loss

#     def backward(self):
#         dout = self.loss_layer.backward()  # 逆伝播 (Softmax)
#         dout = np.dot(dout, self.fc2_W.T)  # 全結合2の逆伝播
#         dout = np.dot(dout, self.fc1_W.T).reshape(self.pool_out.shape)  # 全結合1の逆伝播
#         return dout

class CNN:
  def __init__(self):
    # 初期化
    self.conv = Convolution(
        W=np.random.randn(16, 1, 3, 3) * 0.01,  # 16個のフィルタ (3x3)
        b=np.zeros(16),
        stride=1,
        pad=1
    )
    self.pool = Pooling(pool_h=2, pool_w=2)
    print(f"Pooling initialized with stride: {self.pool.stride}")  # 追加
    self.fc1_W = np.random.randn(
        16 * 27 * 27, 100) * 0.01  # 全結合 (14*14*16 → 100)
    self.fc1_b = np.zeros(100)
    self.fc2_W = np.random.randn(100, 10) * 0.01  # 出力層 (100 → 10)
    self.fc2_b = np.zeros(10)
    self.loss_layer = SoftmaxWithLoss()

  # def forward(self, x, t):
  #     self.x = x
  #     self.t = t
  #     self.conv_out = self.conv.forward(x)  # 畳み込み
  #     self.pool_out = self.pool.forward(self.conv_out)  # プーリング
  #     self.fc_input = self.pool_out.reshape(x.shape[0], -1)  # Flatten
  #     self.fc1_out = np.dot(self.fc_input, self.fc1_W) + self.fc1_b  # 全結合1
  #     self.fc2_out = np.dot(self.fc1_out, self.fc2_W) + self.fc2_b  # 全結合2
  #     self.loss = self.loss_layer.forward(self.fc2_out, t)  # Softmax & Loss
  #     return self.loss

  def backward(self):
    dout = self.loss_layer.backward()  # 逆伝播 (Softmax)

    # fc2の勾配計算
    dfc2_W = np.dot(self.fc1_out.T, dout)
    dfc2_b = np.sum(dout, axis=0)

    # fc1の勾配計算
    dout = np.dot(dout, self.fc2_W.T)
    dfc1_W = np.dot(self.fc_input.T, dout)
    dfc1_b = np.sum(dout, axis=0)

    grads = {
        'fc1_W': dfc1_W,
        'fc1_b': dfc1_b,
        'fc2_W': dfc2_W,
        'fc2_b': dfc2_b
    }

    return grads

  def forward(self, x, t=None):
    # 順伝播の処理（畳み込み、プーリング、全結合層など）
    # 最後に出力を返します
    # 出力層（fc2_out）が最終的な予測結果です
    # ここではtがNoneの場合は評価モードと考えて、損失計算をしません
    self.fc2_out = self.fc2(self.pool2(
        self.conv2(self.pool1(self.conv1(x)))))  # 最終出力
    if t is not None:
      # 損失計算
      loss = self.loss(self.fc2_out, t)
      return loss
    return self.fc2_out

  def predict(self, x):
    # フォワードパスで最終的な出力を取得し、最も確率の高いラベルを予測
    output = self.forward(x)
    prediction = np.argmax(output, axis=1)  # 最も高い出力を持つインデックスを取得
    return prediction

# ハイパーパラメータ
epochs = 1  # 5
batch_size = 50  # 100
learning_rate = 0.01

x_train_subset = x_train[:1000]  # 最初の1000件を使用
t_train_subset = t_train[:1000]

# モデルの初期化
model = CNN()

# 学習ループ
for epoch in range(epochs):
  # シャッフル
  indices = np.random.permutation(len(x_train))
  x_train = x_train[indices]
  t_train = t_train[indices]

  # ミニバッチごとに学習
  for i in range(0, len(x_train), batch_size):
    x_batch = x_train[i:i + batch_size]
    t_batch = t_train[i:i + batch_size]

    # 順伝播
    loss = model.forward(x_batch, t_batch)

    # 逆伝播
    grads = model.backward()

    # パラメータ更新
    model.fc1_W -= learning_rate * grads['fc1_W']
    model.fc1_b -= learning_rate * grads['fc1_b']
    model.fc2_W -= learning_rate * grads['fc2_W']
    model.fc2_b -= learning_rate * grads['fc2_b']

  print(f"Epoch {epoch+1}/{epochs}, Loss: {loss}")

# # ワンホットエンコーディングを適用
# t_test_subset = one_hot_encoding(y_test_subset, 10)

# # 予測を行い、損失と精度を計算する関数
# def evaluate(model, x_test, y_test):
#     loss = 0
#     correct_count = 0
#     for i in range(len(x_test)):
#         x = x_test[i:i+1]  # テストデータのバッチ
#         t = y_test[i:i+1]

#         # フォワードパス
#         loss = model.forward(x, t)

#         # 出力の最大値を取る
#         prediction = np.argmax(model.fc2_out, axis=1)

#         # 正解と予測を比較
#         if prediction == np.argmax(t, axis=1):
#             correct_count += 1

#     # 損失と精度を計算
#     accuracy = correct_count / len(x_test)
#     return loss, accuracy

# # モデルのインスタンスを作成
# model = CNN()

# # テストデータの一部で評価（最初の100枚を使う）
# x_test_subset = x_test[:100]
# y_test_subset = y_test[:100]

# # モデルで評価
# test_loss, test_accuracy = evaluate(model, x_test_subset, y_test_subset)

# print(f"Test Loss: {test_loss}")
# print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# # テストデータでの評価
# test_loss, test_accuracy = evaluate(model, x_test, y_test)

# print(f"Test Loss: {test_loss}")
# print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

def evaluate(model, x_test, y_test):
  correct_count = 0
  total_loss = 0
  for i in range(len(x_test)):
    x = x_test[i:i + 1]  # バッチサイズ1のサンプルを取得
    t = y_test[i:i + 1]

    # モデルで損失計算
    loss = model.forward(x, t)
    total_loss += loss

    # 予測を行う
    prediction = model.predict(x)

    # 正解と予測を比較
    if prediction == np.argmax(t, axis=1):
      correct_count += 1

  # 損失と精度を計算
  accuracy = correct_count / len(x_test)
  return total_loss / len(x_test), accuracy

# def evaluate(model, x_test, y_test):
#     correct_count = 0
#     total_count = len(x_test)

#     for i in range(total_count):
#         x = x_test[i:i+1]  # バッチサイズを1にする
#         t = y_test[i:i+1]  # バッチサイズを1にする

#         # モデルで予測
#         prediction = model.predict(x)

#         # tがワンホットエンコーディングされている場合
#         if len(t.shape) > 1:  # tが2次元のワンホットエンコーディングの場合
#             if np.argmax(prediction) == np.argmax(t):
#                 correct_count += 1
#         else:  # tが1次元の整数ラベルの場合
#             if np.argmax(prediction) == t:
#                 correct_count += 1

#     accuracy = correct_count / total_count
#     loss = model.evaluate(x_test, y_test)  # モデルの評価（損失と精度を取得）

#     return loss[0], accuracy  # 損失と精度を返す

# モデルのインスタンスを作成
model = CNN()

# テストデータの一部で評価（最初の100枚を使う）
x_test_subset = x_test[:100]
y_test_subset = y_test[:100]

# モデルで評価
test_loss, test_accuracy = evaluate(model, x_test_subset, y_test_subset)

print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
